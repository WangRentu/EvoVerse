#!/usr/bin/env python3
"""
EvoVerse å­¦ä¹ è®°å¿†ç³»ç»ŸéªŒè¯è„šæœ¬
å±•ç¤ºå­¦ä¹ æ•ˆæœå’Œè®°å¿†å†…å®¹
"""

import sys
from pathlib import Path
from datetime import datetime
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from evoverse.memory import MemoryStore, MemoryCategory
from evoverse.config import get_config


def print_section(title: str, char: str = "="):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print(f"\n{char * 70}")
    print(f"  {title}")
    print(f"{char * 70}\n")


def print_memory(memory, index: int = None):
    """æ ¼å¼åŒ–æ‰“å°è®°å¿†"""
    prefix = f"[{index}] " if index is not None else ""
    print(f"{prefix}ğŸ“Œ {memory.content}")
    print(f"    â”œâ”€ ç±»åˆ«: {memory.category.value}")
    print(f"    â”œâ”€ é‡è¦æ€§: {memory.importance:.2f}")
    print(f"    â”œâ”€ è®¿é—®æ¬¡æ•°: {memory.access_count}")
    print(f"    â”œâ”€ æ ‡ç­¾: {', '.join(memory.tags) if memory.tags else 'æ— '}")
    print(f"    â”œâ”€ åˆ›å»ºæ—¶é—´: {memory.created_at.strftime('%Y-%m-%d %H:%M:%S')}")
    if memory.data:
        print(f"    â””â”€ æ•°æ®: {json.dumps(memory.data, ensure_ascii=False, indent=6)}")
    else:
        print(f"    â””â”€ æ•°æ®: æ— ")
    print()


def test_learning_memory():
    """æµ‹è¯•å­¦ä¹ è®°å¿†ç³»ç»Ÿ"""
    print_section("ğŸ§  EvoVerse å­¦ä¹ è®°å¿†ç³»ç»ŸéªŒè¯", "=")
    
    # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
    memory_store = MemoryStore(max_memories=100)
    
    print("âœ… è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n")
    
    # ========================================================================
    # é˜¶æ®µ1: æ·»åŠ å„ç§ç±»å‹çš„è®°å¿†
    # ========================================================================
    print_section("é˜¶æ®µ 1: æ·»åŠ å­¦ä¹ è®°å¿†", "-")
    
    print("ğŸ“ æ·»åŠ æˆåŠŸæ¨¡å¼...")
    success_ids = []
    success_ids.append(memory_store.add_success_pattern(
        "ä½¿ç”¨åˆ†æ²»æ³•è§£å†³å¤æ‚é—®é¢˜",
        success_rate=0.9,
        tags=["algorithm", "problem_solving", "divide_conquer"]
    ))
    success_ids.append(memory_store.add_success_pattern(
        "ä½¿ç”¨ç¼“å­˜æœºåˆ¶ä¼˜åŒ–é‡å¤è®¡ç®—",
        success_rate=0.85,
        tags=["optimization", "cache", "performance"]
    ))
    success_ids.append(memory_store.add_success_pattern(
        "ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œè¯­ä¹‰æœç´¢",
        success_rate=0.95,
        tags=["search", "vector_db", "semantic"]
    ))
    print(f"   âœ… æ·»åŠ äº† {len(success_ids)} ä¸ªæˆåŠŸæ¨¡å¼\n")
    
    print("ğŸ“ æ·»åŠ å¤±è´¥æ•™è®­...")
    failure_ids = []
    failure_ids.append(memory_store.add_failure_pattern(
        "å°è¯•æš´åŠ›æšä¸¾å¤§æ•°æ®é›†",
        "å¯¼è‡´å†…å­˜æº¢å‡ºå’Œæ€§èƒ½é—®é¢˜ï¼Œåº”è¯¥ä½¿ç”¨æµå¼å¤„ç†æˆ–åˆ†æ‰¹å¤„ç†",
        tags=["performance", "mistake", "memory"]
    ))
    failure_ids.append(memory_store.add_failure_pattern(
        "åœ¨å¾ªç¯ä¸­é¢‘ç¹è°ƒç”¨ LLM API",
        "å¯¼è‡´APIé™æµå’Œæˆæœ¬è¿‡é«˜ï¼Œåº”è¯¥æ‰¹é‡å¤„ç†æˆ–ä½¿ç”¨ç¼“å­˜",
        tags=["api", "cost", "rate_limit"]
    ))
    print(f"   âœ… æ·»åŠ äº† {len(failure_ids)} ä¸ªå¤±è´¥æ•™è®­\n")
    
    print("ğŸ“ æ·»åŠ æ­»èƒ¡åŒ...")
    dead_end_ids = []
    dead_end_ids.append(memory_store.add_dead_end(
        "å°è¯•ç”¨çº¿æ€§æ¨¡å‹æ‹Ÿåˆéçº¿æ€§å…³ç³»",
        "å¤šæ¬¡å®éªŒè¯æ˜æ•ˆæœå¾ˆå·®ï¼Œåº”è¯¥ä½¿ç”¨éçº¿æ€§æ¨¡å‹æˆ–ç¥ç»ç½‘ç»œ",
        tags=["modeling", "linear", "avoid"]
    ))
    dead_end_ids.append(memory_store.add_dead_end(
        "ä½¿ç”¨å•ä¸€ç‰¹å¾è¿›è¡Œé¢„æµ‹",
        "å‡†ç¡®ç‡å§‹ç»ˆä½äº50%ï¼Œéœ€è¦ç‰¹å¾å·¥ç¨‹å’Œå¤šç‰¹å¾ç»„åˆ",
        tags=["feature", "prediction", "avoid"]
    ))
    print(f"   âœ… æ·»åŠ äº† {len(dead_end_ids)} ä¸ªæ­»èƒ¡åŒ\n")
    
    print("ğŸ“ æ·»åŠ é‡è¦æ´å¯Ÿ...")
    insight_ids = []
    insight_ids.append(memory_store.add_insight(
        "ç¥ç»ç½‘ç»œçš„æ·±åº¦æ¯”å®½åº¦æ›´é‡è¦ï¼Œåœ¨ç›¸åŒå‚æ•°é‡ä¸‹ï¼Œæ·±å±‚ç½‘ç»œé€šå¸¸è¡¨ç°æ›´å¥½",
        "literature_review",
        ["deep_learning", "architecture", "neural_network"]
    ))
    insight_ids.append(memory_store.add_insight(
        "æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æ˜¾è‘—æå‡åºåˆ—æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿åºåˆ—ä»»åŠ¡ä¸­",
        "experiment",
        ["attention", "transformer", "sequence"]
    ))
    insight_ids.append(memory_store.add_insight(
        "æ•°æ®è´¨é‡æ¯”æ•°æ®é‡æ›´é‡è¦ï¼Œé«˜è´¨é‡çš„å°æ•°æ®é›†å¾€å¾€ä¼˜äºä½è´¨é‡çš„å¤§æ•°æ®é›†",
        "research",
        ["data", "quality", "dataset"]
    ))
    print(f"   âœ… æ·»åŠ äº† {len(insight_ids)} ä¸ªé‡è¦æ´å¯Ÿ\n")
    
    # ========================================================================
    # é˜¶æ®µ2: æŸ¥çœ‹æ‰€æœ‰è®°å¿†å†…å®¹
    # ========================================================================
    print_section("é˜¶æ®µ 2: æŸ¥çœ‹è®°å¿†å†…å®¹", "-")
    
    print("ğŸ“š æˆåŠŸæ¨¡å¼è®°å¿†:")
    successes = memory_store.query_memory(MemoryCategory.SUCCESS_PATTERNS, limit=10)
    for i, mem in enumerate(successes, 1):
        print_memory(mem, i)
    
    print("\nğŸ“š å¤±è´¥æ•™è®­è®°å¿†:")
    failures = memory_store.query_memory(MemoryCategory.FAILURE_PATTERNS, limit=10)
    for i, mem in enumerate(failures, 1):
        print_memory(mem, i)
    
    print("\nğŸ“š æ­»èƒ¡åŒè®°å¿†:")
    dead_ends = memory_store.query_memory(MemoryCategory.DEAD_ENDS, limit=10)
    for i, mem in enumerate(dead_ends, 1):
        print_memory(mem, i)
    
    print("\nğŸ“š é‡è¦æ´å¯Ÿè®°å¿†:")
    insights = memory_store.query_memory(MemoryCategory.INSIGHTS, limit=10)
    for i, mem in enumerate(insights, 1):
        print_memory(mem, i)
    
    # ========================================================================
    # é˜¶æ®µ3: å±•ç¤ºå­¦ä¹ æ•ˆæœ - æŸ¥è¯¢ç›¸å…³è®°å¿†
    # ========================================================================
    print_section("é˜¶æ®µ 3: å­¦ä¹ æ•ˆæœå±•ç¤º - æ™ºèƒ½æŸ¥è¯¢", "-")
    
    print("ğŸ” æŸ¥è¯¢ä¸ 'ä¼˜åŒ–' ç›¸å…³çš„è®°å¿†:")
    optimization_memories = memory_store.search_similar("ä¼˜åŒ– performance cache", limit=5)
    for i, mem in enumerate(optimization_memories, 1):
        print(f"  [{i}] {mem.content} (ç±»åˆ«: {mem.category.value}, é‡è¦æ€§: {mem.importance:.2f})")
    print()
    
    print("ğŸ” æŸ¥è¯¢ä¸ 'æ¨¡å‹' ç›¸å…³çš„è®°å¿†:")
    model_memories = memory_store.search_similar("æ¨¡å‹ neural network deep learning", limit=5)
    for i, mem in enumerate(model_memories, 1):
        print(f"  [{i}] {mem.content} (ç±»åˆ«: {mem.category.value}, é‡è¦æ€§: {mem.importance:.2f})")
    print()
    
    print("ğŸ” æŸ¥è¯¢é«˜é‡è¦æ€§è®°å¿† (é‡è¦æ€§ >= 0.9):")
    important_memories = memory_store.query_memory(min_importance=0.9, limit=10)
    for i, mem in enumerate(important_memories, 1):
        print(f"  [{i}] {mem.content} (é‡è¦æ€§: {mem.importance:.2f}, ç±»åˆ«: {mem.category.value})")
    print()
    
    # ========================================================================
    # é˜¶æ®µ4: å®éªŒå»é‡åŠŸèƒ½
    # ========================================================================
    print_section("é˜¶æ®µ 4: å®éªŒå»é‡åŠŸèƒ½", "-")
    
    print("ğŸ§ª è®°å½•å®éªŒ...")
    exp1_hash = memory_store.record_experiment(
        "ä½¿ç”¨BERTè¿›è¡Œæ–‡æœ¬åˆ†ç±»",
        "fine-tuning with learning rate 2e-5"
    )
    print(f"   âœ… å®éªŒ1å·²è®°å½•: {exp1_hash[:16]}...")
    
    exp2_hash = memory_store.record_experiment(
        "ä½¿ç”¨GPTè¿›è¡Œæ–‡æœ¬ç”Ÿæˆ",
        "few-shot learning with 5 examples"
    )
    print(f"   âœ… å®éªŒ2å·²è®°å½•: {exp2_hash[:16]}...")
    
    print("\nğŸ” æ£€æŸ¥é‡å¤å®éªŒ...")
    
    # æ£€æŸ¥ç›¸åŒå®éªŒ
    is_dup1, reason1 = memory_store.is_duplicate_experiment(
        "ä½¿ç”¨BERTè¿›è¡Œæ–‡æœ¬åˆ†ç±»",
        "fine-tuning with learning rate 2e-5"
    )
    print(f"   å®éªŒ1é‡å¤æ£€æŸ¥: {'âŒ æ˜¯é‡å¤å®éªŒ' if is_dup1 else 'âœ… æ–°å®éªŒ'}")
    if reason1:
        print(f"   åŸå› : {reason1}")
    
    # æ£€æŸ¥æ–°å®éªŒ
    is_dup2, reason2 = memory_store.is_duplicate_experiment(
        "ä½¿ç”¨RoBERTaè¿›è¡Œæ–‡æœ¬åˆ†ç±»",
        "fine-tuning with learning rate 1e-5"
    )
    print(f"   å®éªŒ3é‡å¤æ£€æŸ¥: {'âŒ æ˜¯é‡å¤å®éªŒ' if is_dup2 else 'âœ… æ–°å®éªŒ'}")
    if reason2:
        print(f"   åŸå› : {reason2}")
    
    # ========================================================================
    # é˜¶æ®µ5: å±•ç¤ºå­¦ä¹ æ•ˆæœ - é¿å…é‡å¤é”™è¯¯
    # ========================================================================
    print_section("é˜¶æ®µ 5: å­¦ä¹ æ•ˆæœ - é¿å…é‡å¤é”™è¯¯", "-")
    
    print("ğŸ’¡ åœºæ™¯: ç³»ç»Ÿè¦å¤„ç†å¤§æ•°æ®é›†ï¼ŒæŸ¥è¯¢ç›¸å…³è®°å¿†...")
    big_data_memories = memory_store.search_similar("å¤§æ•°æ®é›† å†…å­˜ å¤„ç†", limit=3)
    
    if big_data_memories:
        print("\n   âš ï¸  å‘ç°ç›¸å…³å¤±è´¥è®°å¿†:")
        for mem in big_data_memories:
            if mem.category == MemoryCategory.FAILURE_PATTERNS:
                print(f"      - {mem.content}")
                print(f"        æ•™è®­: {mem.data.get('lesson', 'N/A')}")
        print("\n   âœ… ç³»ç»Ÿå¯ä»¥é¿å…é‡å¤è¿™ä¸ªé”™è¯¯ï¼")
    else:
        print("   â„¹ï¸  æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
    print()
    
    print("ğŸ’¡ åœºæ™¯: ç³»ç»Ÿè¦ä¼˜åŒ–æ€§èƒ½ï¼ŒæŸ¥è¯¢ç›¸å…³è®°å¿†...")
    perf_memories = memory_store.query_memory(
        tags=["performance", "optimization"],
        limit=5
    )
    
    if perf_memories:
        print("\n   ğŸ“– æ‰¾åˆ°ç›¸å…³è®°å¿†:")
        for mem in perf_memories:
            category_icon = "âœ…" if mem.category == MemoryCategory.SUCCESS_PATTERNS else "âŒ"
            print(f"      {category_icon} {mem.content}")
        print("\n   âœ… ç³»ç»Ÿå¯ä»¥å‚è€ƒè¿™äº›ç»éªŒï¼")
    else:
        print("   â„¹ï¸  æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
    print()
    
    # ========================================================================
    # é˜¶æ®µ6: è®°å¿†ç»Ÿè®¡å’Œè¯¦ç»†ä¿¡æ¯
    # ========================================================================
    print_section("é˜¶æ®µ 6: è®°å¿†ç»Ÿè®¡ä¿¡æ¯", "-")
    
    stats = memory_store.get_stats()
    print("ğŸ“Š è®°å¿†ç³»ç»Ÿç»Ÿè®¡:")
    print(f"   æ€»è®°å¿†æ•°: {stats['total_memories']}")
    print(f"   æœ€å¤§å®¹é‡: {stats['max_memories']}")
    print(f"   å®éªŒç­¾åæ•°: {stats['experiment_signatures']}")
    print(f"   æ¸…ç†å‘¨æœŸ: {stats['prune_after_days']} å¤©")
    print()
    
    print("ğŸ“Š æŒ‰ç±»åˆ«ç»Ÿè®¡:")
    for category, count in stats['by_category'].items():
        print(f"   {category}: {count} æ¡")
    print()
    
    # ========================================================================
    # é˜¶æ®µ7: å±•ç¤ºè®°å¿†è®¿é—®è¿½è¸ª
    # ========================================================================
    print_section("é˜¶æ®µ 7: è®°å¿†è®¿é—®è¿½è¸ª", "-")
    
    print("ğŸ” å¤šæ¬¡æŸ¥è¯¢åŒä¸€è®°å¿†ï¼Œè§‚å¯Ÿè®¿é—®è®¡æ•°å˜åŒ–...")
    
    # æŸ¥è¯¢å‡ æ¬¡ï¼Œå¢åŠ è®¿é—®è®¡æ•°
    for i in range(3):
        memory_store.query_memory(MemoryCategory.INSIGHTS, limit=1)
    
    # å†æ¬¡æŸ¥è¯¢å¹¶æ˜¾ç¤ºè®¿é—®è®¡æ•°
    accessed_memories = memory_store.query_memory(MemoryCategory.INSIGHTS, limit=5)
    print("\n   è®¿é—®è®¡æ•°æœ€é«˜çš„æ´å¯Ÿ:")
    for mem in sorted(accessed_memories, key=lambda m: m.access_count, reverse=True)[:3]:
        print(f"      - {mem.content[:60]}...")
        print(f"        è®¿é—®æ¬¡æ•°: {mem.access_count}, æœ€åè®¿é—®: {mem.last_accessed.strftime('%H:%M:%S')}")
    print()
    
    # ========================================================================
    # é˜¶æ®µ8: å¯¼å‡ºè®°å¿†å†…å®¹
    # ========================================================================
    print_section("é˜¶æ®µ 8: å¯¼å‡ºè®°å¿†å†…å®¹", "-")
    
    print("ğŸ’¾ å¯¼å‡ºæ‰€æœ‰è®°å¿†ä¸º JSON æ ¼å¼...")
    all_memories = []
    for category in MemoryCategory:
        memories = memory_store.query_memory(category, limit=100)
        for mem in memories:
            all_memories.append({
                "id": mem.id,
                "category": mem.category.value,
                "content": mem.content,
                "importance": mem.importance,
                "tags": mem.tags,
                "access_count": mem.access_count,
                "created_at": mem.created_at.isoformat(),
                "data": mem.data
            })
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = Path("memory_export.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_memories, f, ensure_ascii=False, indent=2)
    
    print(f"   âœ… å·²å¯¼å‡º {len(all_memories)} æ¡è®°å¿†åˆ°: {output_file}")
    print(f"   ğŸ“„ æ–‡ä»¶å¤§å°: {output_file.stat().st_size} å­—èŠ‚")
    print()
    
    # ========================================================================
    # æ€»ç»“
    # ========================================================================
    print_section("âœ… éªŒè¯å®Œæˆ", "=")
    
    print("ğŸ“‹ éªŒè¯æ€»ç»“:")
    print(f"   âœ… æˆåŠŸæ·»åŠ  {len(success_ids)} ä¸ªæˆåŠŸæ¨¡å¼")
    print(f"   âœ… æˆåŠŸæ·»åŠ  {len(failure_ids)} ä¸ªå¤±è´¥æ•™è®­")
    print(f"   âœ… æˆåŠŸæ·»åŠ  {len(dead_end_ids)} ä¸ªæ­»èƒ¡åŒ")
    print(f"   âœ… æˆåŠŸæ·»åŠ  {len(insight_ids)} ä¸ªé‡è¦æ´å¯Ÿ")
    print(f"   âœ… è®°å½•äº† {stats['experiment_signatures']} ä¸ªå®éªŒç­¾å")
    print(f"   âœ… æ€»è®°å¿†æ•°: {stats['total_memories']}")
    print()
    
    print("ğŸ¯ å­¦ä¹ è®°å¿†ç³»ç»ŸåŠŸèƒ½éªŒè¯:")
    print("   âœ… è®°å¿†æ·»åŠ åŠŸèƒ½æ­£å¸¸")
    print("   âœ… è®°å¿†æŸ¥è¯¢åŠŸèƒ½æ­£å¸¸")
    print("   âœ… ç›¸ä¼¼è®°å¿†æœç´¢æ­£å¸¸")
    print("   âœ… å®éªŒå»é‡åŠŸèƒ½æ­£å¸¸")
    print("   âœ… è®¿é—®è®¡æ•°è¿½è¸ªæ­£å¸¸")
    print("   âœ… è®°å¿†å¯¼å‡ºåŠŸèƒ½æ­£å¸¸")
    print()
    
    print("ğŸ’¡ å­¦ä¹ æ•ˆæœ:")
    print("   âœ… ç³»ç»Ÿå¯ä»¥è®°ä½æˆåŠŸçš„æ–¹æ³•")
    print("   âœ… ç³»ç»Ÿå¯ä»¥è®°ä½å¤±è´¥çš„æ•™è®­")
    print("   âœ… ç³»ç»Ÿå¯ä»¥é¿å…é‡å¤é”™è¯¯")
    print("   âœ… ç³»ç»Ÿå¯ä»¥æ£€ç´¢ç›¸å…³ç»éªŒ")
    print("   âœ… ç³»ç»Ÿå¯ä»¥é˜²æ­¢é‡å¤å®éªŒ")
    print()


if __name__ == "__main__":
    try:
        test_learning_memory()
    except Exception as e:
        print(f"\nâŒ éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()