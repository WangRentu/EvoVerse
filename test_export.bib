@article{shin2024,
  title = {Large Language Models Lack Understanding of Character Composition of Words},
  author = {Andrew Shin and Kunitake Kaneko},
  year = {2024},
  url = {http://arxiv.org/abs/2405.11357v3},
  abstract = {Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs' successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple tasks that can be handled by humans with perfection. We analyze their behaviors with comparison to token level performances, and discuss the potential directions for future research.},
}

@article{guo2024,
  title = {Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models},
  author = {Linge Guo},
  year = {2024},
  url = {http://arxiv.org/abs/2403.09676v1},
  abstract = {This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reasoning, along with the social implications and risks they entail. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI. This encompasses considerations of international collaborative governance, the reconfigured engagement of individuals with AI, proposal of practical adjustments, and specific elements of digital education.},
}

@article{ai2024,
  title = {Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality},
  author = {Yiming Ai and Zhiwei He and Ziyin Zhang and Wenhong Zhu and Hongkun Hao and Kai Yu and Lingjun Chen and Rui Wang},
  year = {2024},
  url = {http://arxiv.org/abs/2402.14679v2},
  abstract = {In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to understand the disjunction between self-knowledge and action in LLMs.},
}

