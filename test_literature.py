#!/usr/bin/env python3
"""
EvoVerse Literature System Complete Test Script

å±•ç¤ºå®Œæ•´çš„æ–‡çŒ®çˆ¬å–ã€ç»“æ„åŒ–æå–ã€å­˜å‚¨å’Œå±•ç¤ºæµç¨‹
"""

import sys
import os
import json
import logging
from pathlib import Path
from typing import List, Dict, Any

# Add EvoVerse to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def display_paper_metadata(paper, index: int = 0):
    """å®Œæ•´å±•ç¤ºè®ºæ–‡å…ƒæ•°æ®ç»“æ„"""
    print(f"\n{'='*80}")
    print(f"ğŸ“„ è®ºæ–‡ {index + 1}: {paper.title}")
    print(f"{'='*80}")
    
    # åŸºæœ¬æ ‡è¯†ä¿¡æ¯
    print("ğŸ”– æ ‡è¯†ä¿¡æ¯:")
    print(f"  â€¢ ID: {paper.id}")
    print(f"  â€¢ æ¥æº: {paper.source.value}")
    if paper.doi:
        print(f"  â€¢ DOI: {paper.doi}")
    if paper.arxiv_id:
        print(f"  â€¢ arXiv ID: {paper.arxiv_id}")
    if paper.pubmed_id:
        print(f"  â€¢ PubMed ID: {paper.pubmed_id}")
    
    # ä½œè€…ä¿¡æ¯
    if paper.authors:
        print(f"\nğŸ‘¥ ä½œè€…ä¿¡æ¯ ({len(paper.authors)} äºº):")
        for i, author in enumerate(paper.authors[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ªä½œè€…
            affiliation = f" ({author.affiliation})" if author.affiliation else ""
            email = f" <{author.email}>" if author.email else ""
            print(f"  {i}. {author.name}{affiliation}{email}")
        if len(paper.authors) > 5:
            print(f"  ... è¿˜æœ‰ {len(paper.authors) - 5} ä½ä½œè€…")
    
    # å‡ºç‰ˆä¿¡æ¯
    print("ğŸ“… å‡ºç‰ˆä¿¡æ¯:")    
    if paper.publication_date:
        print(f"  â€¢ å‘è¡¨æ—¥æœŸ: {paper.publication_date.strftime('%Y-%m-%d')}")
    if paper.year:
        print(f"  â€¢ å¹´ä»½: {paper.year}")
    if paper.journal:
        print(f"  â€¢ æœŸåˆŠ: {paper.journal}")
    if paper.venue:
        print(f"  â€¢ ä¼šè®®/åœºæ‰€: {paper.venue}")
    
    # é“¾æ¥èµ„æº
    print("ğŸ”— èµ„æºé“¾æ¥:")
    if paper.url:
        print(f"  â€¢ é¡µé¢é“¾æ¥: {paper.url}")
    if paper.pdf_url:
        print(f"  â€¢ PDFé“¾æ¥: {paper.pdf_url}")
    
    # å¼•ç”¨å½±å“åŠ›
    print("ğŸ“Š å¼•ç”¨ç»Ÿè®¡:")
    print(f"  â€¢ æ€»å¼•ç”¨æ•°: {paper.citation_count}")
    print(f"  â€¢ å‚è€ƒæ–‡çŒ®æ•°: {paper.reference_count}")
    print(f"  â€¢ æœ‰å½±å“åŠ›å¼•ç”¨æ•°: {paper.influential_citation_count}")
    
    # ç ”ç©¶é¢†åŸŸå’Œå…³é”®è¯
    if paper.fields:
        print(f"\nğŸ·ï¸ ç ”ç©¶é¢†åŸŸ ({len(paper.fields)} ä¸ª):")
        for field in paper.fields[:5]:
            print(f"  â€¢ {field}")
        if len(paper.fields) > 5:
            print(f"  ... è¿˜æœ‰ {len(paper.fields) - 5} ä¸ªé¢†åŸŸ")
    
    if paper.keywords:
        print(f"\nğŸ”‘ å…³é”®è¯ ({len(paper.keywords)} ä¸ª):")
        print(f"  â€¢ {', '.join(paper.keywords[:10])}")
        if len(paper.keywords) > 10:
            print(f"  ... è¿˜æœ‰ {len(paper.keywords) - 10} ä¸ªå…³é”®è¯")
    
    # æ‘˜è¦
    if paper.abstract:
        print("ğŸ“ æ‘˜è¦:")        # é™åˆ¶æ‘˜è¦é•¿åº¦ä»¥ä¿æŒå¯è¯»æ€§
        abstract_preview = paper.abstract[:500] + "..." if len(paper.abstract) > 500 else paper.abstract
        print(f"  {abstract_preview}")
    
    # å…¨æ–‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    if paper.full_text:
        print("ğŸ“– å…¨æ–‡é¢„è§ˆ:")
        text_preview = paper.full_text[:300] + "..." if len(paper.full_text) > 300 else paper.full_text
        print(f"  {text_preview}")
        print(f"  ğŸ“ æ€»å­—æ•°: {len(paper.full_text)}")

def test_complete_literature_workflow():
    """å®Œæ•´çš„æ–‡çŒ®å·¥ä½œæµæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹å®Œæ•´çš„ EvoVerse æ–‡çŒ®ç³»ç»Ÿæµ‹è¯•\n")
    
    try:
        # 1. å¯¼å…¥æ‰€æœ‰å¿…è¦æ¨¡å—
        print("ğŸ“¦ æ­¥éª¤1: å¯¼å…¥æ¨¡å—...")
        from evoverse.literature import (
            UnifiedLiteratureSearch, 
            ReferenceManager, 
            PDFExtractor,
            PaperSource
        )
        print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # 2. åˆ›å»ºç»Ÿä¸€æœç´¢å™¨
        print("\nğŸ” æ­¥éª¤2: åˆå§‹åŒ–ç»Ÿä¸€æœç´¢å™¨...")
        searcher = UnifiedLiteratureSearch(
            arxiv_enabled=True,
            semantic_scholar_enabled=True,
            pubmed_enabled=False,  # å…ˆç¦ç”¨PubMedé¿å…APIé™åˆ¶
            # max_results_per_source=3  # æ¯ä¸ªæºé™åˆ¶3ç¯‡
        )
        print("âœ… æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # 3. æ‰§è¡Œæ–‡çŒ®æœç´¢
        query = "large language models transformers"
        print(f"\nğŸ“š æ­¥éª¤3: æ‰§è¡Œæ–‡çŒ®æœç´¢ - æŸ¥è¯¢: '{query}'")
        
        papers = searcher.search(
            query=query,
            max_results_per_source=3,
            extract_full_text=False  # å…ˆä¸æå–å…¨æ–‡ï¼ŒèŠ‚çœæ—¶é—´
        )
        
        print(f"âœ… æœç´¢å®Œæˆï¼Œå…±è·å– {len(papers)} ç¯‡è®ºæ–‡")
        
        # 4. è¯¦ç»†å±•ç¤ºæ¯ç¯‡è®ºæ–‡çš„ç»“æ„åŒ–ä¿¡æ¯
        print("ğŸ“‹ æ­¥éª¤4: å±•ç¤ºç»“æ„åŒ–æå–ç»“æœ...")
        for i, paper in enumerate(papers):
            display_paper_metadata(paper, i)
        
        # 5. åˆ›å»ºå‚è€ƒæ–‡çŒ®ç®¡ç†å™¨å¹¶å­˜å‚¨
        if papers:
            print("ğŸ’¾ æ­¥éª¤5: åˆ›å»ºå‚è€ƒæ–‡çŒ®åº“å¹¶æŒä¹…åŒ–...")
            manager = ReferenceManager(storage_path="test_complete_library.json")

            # æ·»åŠ è®ºæ–‡åˆ°åº“
            ref_ids = manager.add_references(papers)
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(ref_ids)} ç¯‡è®ºæ–‡åˆ°å‚è€ƒæ–‡çŒ®åº“")
            
            # æ˜¾ç¤ºåº“ç»Ÿè®¡
            stats = manager.get_statistics()
            print(f"ğŸ“Š åº“ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  â€¢ æ€»è®ºæ–‡æ•°: {stats['total_count']}")
            print(f"  â€¢ åŒ…å«DOI: {stats['doi_count']}")
            print(f"  â€¢ arXivè®ºæ–‡: {stats['arxiv_count']}")
            # print(f"  â€¢ Semantic Scholarè®ºæ–‡: {stats['semantic_scholar_count']}")
            print(f"  â€¢ PubMedè®ºæ–‡: {stats['pubmed_count']}")
            print(f"  â€¢ å¼•ç”¨é“¾æ¥æ•°: {stats['citation_links']}")
            
            # 6. æµ‹è¯•PDFæå–ï¼ˆå¦‚æœæœ‰PDFé“¾æ¥ï¼‰
            print("ğŸ“„ æ­¥éª¤6: æµ‹è¯•PDFæå–...")
            extractor = PDFExtractor()
            pdf_extracted_count = 0
            
            for paper in papers[:2]:  # åªæµ‹è¯•å‰2ç¯‡
                if paper.pdf_url:
                    print(f"ğŸ“¥ å°è¯•æå–PDF: {paper.pdf_url}")
                    text = extractor.extract_from_url(
                        paper.pdf_url, 
                        paper_id=paper.primary_identifier
                    )
                    if text:
                        print(f"âœ… PDFæå–æˆåŠŸï¼Œè·å¾— {len(text)} å­—ç¬¦æ–‡æœ¬")
                        pdf_extracted_count += 1
                        
                        # æ˜¾ç¤ºæ–‡æœ¬é¢„è§ˆ
                        preview = text[:200] + "..." if len(text) > 200 else text
                        print(f"ğŸ“– æ–‡æœ¬é¢„è§ˆ: {preview}")
                    else:
                        print("âŒ PDFæå–å¤±è´¥")
            
            if pdf_extracted_count > 0:
                print(f"ğŸ‰ å…±æˆåŠŸæå– {pdf_extracted_count} ç¯‡PDF")
            else:
                print("â„¹ï¸  æ— å¯ç”¨PDFé“¾æ¥æˆ–æå–å¤±è´¥")
        
        # 7. æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        print("ğŸ“Š æ­¥éª¤7: æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡..." )
        from evoverse.literature import get_cache
        cache = get_cache()
        stats = cache.get_stats()
        
        print(f"ğŸ’¾ ç¼“å­˜ç³»ç»ŸçŠ¶æ€:")
        print(f"  â€¢ ç¼“å­˜ç›®å½•: {stats['cache_dir']}")
        print(f"  â€¢ æ€»æ¡ç›®æ•°: {stats['total_entries']}")
        print(f"  â€¢ ç¼“å­˜å¤§å°: {stats['size_mb']:.2f} MB")
        print(f"  â€¢ è¿‡æœŸæ¡ç›®: {stats['expired_entries']}")
        print(f"  â€¢ TTLè®¾ç½®: {stats['ttl_hours']} å°æ—¶")
        
        # 8. æ˜¾ç¤ºæ–‡ä»¶ç³»ç»ŸçŠ¶æ€
        print("ğŸ—‚ï¸ æ­¥éª¤8: æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶...")
        files_to_check = [
            "test_complete_library.json",
            ".literature_cache"
        ]
        
        for file_path in files_to_check:
            path = Path(file_path)
            if path.exists():
                if path.is_file():
                    size_mb = path.stat().st_size / (1024 * 1024)
                    print(f"âœ… æ–‡ä»¶å­˜åœ¨: {file_path} ({size_mb:.2f} MB)")
                else:
                    # è®¡ç®—ç›®å½•å¤§å°
                    total_size = sum(f.stat().st_size for f in path.rglob("*") if f.is_file())
                    size_mb = total_size / (1024 * 1024)
                    print(f"âœ… ç›®å½•å­˜åœ¨: {file_path} ({size_mb:.2f} MB)")
            else:
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # 9. å¯¼å‡ºä¸ºå…¶ä»–æ ¼å¼
        if papers:
            print("ğŸ“¤ æ­¥éª¤9: å¯¼å‡ºå‚è€ƒæ–‡çŒ®...")
            from evoverse.literature import papers_to_bibtex, papers_to_ris
            
            # BibTeXå¯¼å‡º
            papers_to_bibtex(papers, "test_export.bib")
            print("âœ… å¯¼å‡º BibTeX: test_export.bib")
            
            # RISå¯¼å‡º
            papers_to_ris(papers, "test_export.ris")
            print("âœ… å¯¼å‡º RIS: test_export.ris")
            
            # JSONå¯¼å‡º
            with open("test_export.json", "w", encoding="utf-8") as f:
                json.dump([paper.to_dict() for paper in papers], f, indent=2, ensure_ascii=False)
            print("âœ… å¯¼å‡º JSON: test_export.json")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def cleanup_test_files():
    """æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
    test_files = [
        "test_complete_library.json",
        "test_export.bib",
        "test_export.ris", 
        "test_export.json"
    ]
    
    cleaned = 0
    for file_path in test_files:
        path = Path(file_path)
        if path.exists():
            path.unlink()
            cleaned += 1
    
    # æ¸…ç†PDFç¼“å­˜ï¼ˆå¯é€‰ï¼‰
    pdf_cache = Path(".literature_cache/pdfs")
    if pdf_cache.exists():
        for pdf_file in pdf_cache.glob("*.pdf"):
            pdf_file.unlink()
            cleaned += 1
    
    print(f"ğŸ§¹ æ¸…ç†äº† {cleaned} ä¸ªæµ‹è¯•æ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="EvoVerse æ–‡çŒ®ç³»ç»Ÿå®Œæ•´æµ‹è¯•")
    parser.add_argument("--cleanup", action="store_true", help="æ¸…ç†æµ‹è¯•æ–‡ä»¶")
    parser.add_argument("--query", default="large language models transformers", help="æœç´¢æŸ¥è¯¢")
    parser.add_argument("--max-results", type=int, default=3, help="æ¯æºæœ€å¤§ç»“æœæ•°")
    
    args = parser.parse_args()
    
    if args.cleanup:
        cleanup_test_files()
        return
    
    print("ğŸ¯ EvoVerse æ–‡çŒ®ç³»ç»Ÿå®Œæ•´æµ‹è¯•")
    print("=" * 80)
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    success = test_complete_literature_workflow()
    
    print("\n" + "=" * 80)
    if success:
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  â€¢ test_complete_library.json - å‚è€ƒæ–‡çŒ®åº“")
        print("  â€¢ test_export.bib - BibTeXæ ¼å¼")
        print("  â€¢ test_export.ris - RISæ ¼å¼")
        print("  â€¢ test_export.json - JSONæ ¼å¼")
        print("  â€¢ .literature_cache/ - APIç¼“å­˜ç›®å½•")
        
        print("\nğŸ§¹ è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¸…ç†æµ‹è¯•æ–‡ä»¶:")
        print("  python test_complete_literature.py --cleanup")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–")

if __name__ == "__main__":
    main()